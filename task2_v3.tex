\documentclass{article}


\usepackage[T1, T2A]{fontenc}
\usepackage[russian,english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{float}

\title{Отчет по решению задачи оптимизации методом сопряженных градиентов}
\author{Тишина Ульяна}

\begin{document}

\maketitle

\newpage
\tableofcontents

\newpage
\section{Введение}
В данном отчете рассматривается метод сопряженных градиентов для решения задачи оптимизации квадратичной функции. Описаны постановка задачи, реализация метода на языке Python, и приведены результаты тестовых примеров с визуализацией процесса оптимизации.

\newpage
\section{Постановка задачи}
Рассмотрим задачу оптимизации функции многих переменных (пусть х - вектор, состоящий из нескольких переменных):
\begin{equation}
f(x)  \rightarrow min,
\end{equation}

\section{Метод сопряженных градиентов}
Метод сопряженных градиентов — это итеративный метод, используемый для нахождения минимума функций.

Этот метод хорошо применим для выпуклых функций, поэтому сначала нужна проверка на выпуклость. Необходимо задать гессиан (матрица 2-х частных проиводных) и проверить ее на положительную определенность. Сделаем это так: возьмем несколько точек и проверим знак гессиана в них. Если хотя бы в одной точке гессиан отрицателен, то такая функция не положительно определена. Если такая функция выдает ответ: функция положительно определена, то это не точно, ведь мы могли просто не проверить функцию на тех точках, где она не положительно определена.

\section{Реализация проверки на выпуклость}
Код на Python, реализующий метод сопряженных градиентов, представлен ниже:

\begin{lstlisting}[language=Python]

x = np.linspace(-10, 10, 10)
x_range = np.array(np.meshgrid(x, x, x)).T.reshape(-1, 3)
y_range = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)
check_x_range = np.array(np.meshgrid(x, x, x, x)).T.reshape(-1, 4)

def check_convexity(func, grad_func, hessian_func, x_range):
    for x in x_range:
        H = hessian_func(x)
        eigenvalues = np.linalg.eigvals(H)
        if not np.all(eigenvalues >= 0):
            return False
    return True

\end{lstlisting}



\newpage
\section{Реализация функций над векторами и матрицами}
\begin{lstlisting}[language=Python]
# mult float and vector
def mult(a, arr):
    return np.array([i*a for i in arr])

# add 2 vectors
def add(arr1, arr2):
    try:
        return np.array([arr1[i]+arr2[i] for i in range(len(arr1))])
    except TypeError:
        arr1+arr2[0]

# dot of 2 vectors
def mydot(arr1, arr2):
    try:
        ans=0
        for i in range(len(arr1)):
            ans+=arr1[i]*arr2[i]
        return ans
    except:
        return arr1*arr2[0]
# norm of vector
def norm(arr):
    return sum(i**2 for i in arr)**0.5

# take grad
def my_grad(f, x, h=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        xh = x.copy()
        x_h = x.copy()
        xh[i] += h
        x_h[i] -= h
        grad[i] = (f(xh) - f(x_h))/h/2
    return grad
\end{lstlisting}


\newpage
\section{Реализация метода сопряженных граиентов}
Код на Python, реализующий метод сопряженных градиентов, представлен ниже:

\begin{lstlisting}[language=Python]

def conjugate_gradient5(f, grad_f, x0, tol=1e-6, max_iter=200,
	alpha_max=1.0, c=0.5, rho=0.5, alpha_min=1e-8, max_f = 1e15):

	arr_f = [f(x0)]
    x = x0
    r = mult(-1, my_grad(f, x))
    d = r

    ans = {"message": "ok",
           "status": 0,
           "nit": 0,
           "x": x,
           "f_min": arr_f[-1],
           "arr_f": arr_f}

    for i in range(max_iter):
        ans["nit"]+=1
        alpha = alpha_max
        
        while f(add(x, mult(alpha, d))) > f(x) + c * alpha *
        		mydot(my_grad(f, x), d) and alpha > alpha_min:
            alpha *= rho

        if abs(f(x))>=max_f:
            ans["message"] = 'too big f or too small'
            ans["x"], ans["f_min"], ans["arr_f"], ans["status"] = x, arr_f[-1], arr_f, 1
            return ans
        if alpha < alpha_min: 
            ans["message"] = 'too small alpha'
            ans["x"], ans["f_min"], ans["arr_f"], ans["status"] = x, arr_f[-1], arr_f, 2
            return ans

        x = add(x, mult(alpha, d))
        arr_f.append(f(x))
        r_new = mult(-1, my_grad(f, x))
        beta = mydot(r_new, r_new) / mydot(r, r)
        d = add(r_new, mult(beta, d))
        r = r_new

        if norm(r) < tol:
            break

    ans["x"], ans["f_min"], ans["arr_f"] = x, arr_f[-1], arr_f
    return ans

\end{lstlisting}

\newpage
\section{Проверка работы}

$$f = x^2 + y^2 + z^2,$$

$$g = x^3 + y^3 - 3xy,$$

$$fun3 = -2x_0^3 + 4x_0^2 + x_1^4 - 3x_1^2 + 2*x_2^2 * x_1 - 5x_2 + x_3^4 - x_3^2 * x_0^2 - 10$$

$$fun4 = (x_0+3)^4 + (x_1-1)^2 + (x_2-2)^2 + (x_3+x_0)^2$$


$$fun5 = -2x_0^3+4x_0^2+x_1^4-3x_1^3-4x_2^3+2x_2^4-5x_3+x_3^2+10+x_0+x_1+x_2+x_3$$

\subsection{Пример 1}

\subsubsection{Реализация}
\begin{lstlisting}[language=Python]
def f(x):
  x,y,z = x
  return x**2 + y**2 + z**2

def grad_f(x):
  x,y,z = x
  return np.array([2*x, 2*y, 2*z])

def hessian_f(x):
  x,y,z = x
  return np.array([[2, 0, 0],
                   [0, 2, 0],
                   [0, 0, 2]])
\end{lstlisting}

\subsubsection{Проверка выпуклости}
\begin{lstlisting}[language=Python]
x_range = np.array(np.meshgrid(x, x, x)).T.reshape(-1, 3)
is_convex = check_convexity(f, grad_f, hessian_f, x_range)
print(f\"f(x, y, z) is convex: {is_convex}\")
\end{lstlisting}

Вывод: f(x, y, z) is convex: True

Начальную точку возьмем (1,1,6)

\begin{lstlisting}[language=Python]
x0_1 = np.array([1, 1, 6])
\end{lstlisting}

\subsubsection{Применение МСГ}

\begin{lstlisting}[language=Python]
res1 = conjugate_gradient5(f, grad_f, x0_1)
\end{lstlisting}


\newpage
\subsection{Пример 2}

\subsubsection{Реализация}
\begin{lstlisting}[language=Python]
def g(x):
  x, y = x
  return x**3 + y**3 - 3*x*y

def grad_g(x):
  x, y = x
  return np.array([3*x**2 - 3*y, 3*y**2 - 3*x])

def hessian_g(x):
  x, y = x
  return np.array([[6*x, -3],
                   [-3, 6*y]])
\end{lstlisting}

\subsubsection{Проверка выпуклости}
\begin{lstlisting}[language=Python]
y_range = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)
is_convex = check_convexity(g, grad_g, hessian_g, y_range)
print(f"g(x, y) is convex: {is_convex}")
\end{lstlisting}

g(x, y) is convex: False

Начальную точку возьмем (8,-2)

\begin{lstlisting}[language=Python]
x0_2 = np.array([8, -2])
\end{lstlisting}

\subsubsection{Применение МСГ}
\begin{lstlisting}[language=Python]
res2 = conjugate_gradient5(g, grad_g, x0_2)
\end{lstlisting}

\newpage
\subsection{Пример 3}

\subsubsection{Реализация}
\begin{lstlisting}[language=Python]
def check_fun(x):
    x0,x1,x2,x3=x
    return -2*x0**3 + 4*x0**2 + x1**4 - 3*x1**2 + 2*x2**2 * x1 - 
    		5*x2 + x3**4 - x3**2 * x0**2 - 10
def grad_check_fun(x):
    x0,x1,x2,x3=x
    return np.array([-6*x0**2 + 8*x0 - 2*x3**2 * x0,
            4*x1**3 - 6*x1 + 2*x2**2,
            4*x2 * x1 - 5,
            4*x3**3 - 2*x3 * x0**2])
def hessian_check_fun(x):
    x0,x1,x2,x3=x
    return np.array([[-12*x0 + 8 - 2*x3**2, 0, 0, -4*x3*x0],
            [0, 12*x1**2 - 6, 4*x2, 0],
            [0, 4*x2, 4*x1, 0],
            [-4*x3*x0, 0, 0, 12*x3**2 - 2*x0**2]])
\end{lstlisting}

\subsubsection{Проверка выпуклости}
\begin{lstlisting}[language=Python]
check_x_range = np.array(np.meshgrid(x, x, x, x)).T.reshape(-1, 4)
is_convex = check_convexity(check_fun, grad_check_fun, hessian_check_fun, check_x_range)
print(f"check_fun(x, y, z, k) is convex: {is_convex}")
\end{lstlisting}

Вывод: check\_fun(x, y, z, k) выпуклая: False.

Начальную точку возьмем (-1,-1,-1,-1)

\begin{lstlisting}[language=Python]
check_x_0 = [-1,-1,-1,-1]
\end{lstlisting}

\subsubsection{Применение МСГ}

\begin{lstlisting}[language=Python]
check_res = conjugate_gradient5(check_fun, grad_check_fun, check_x_0)
\end{lstlisting}

\newpage
\subsection{Пример 4}

\subsubsection{Реализация}
\begin{lstlisting}[language=Python]
def fun4(x):
    x0, x1, x2, x3 = x
    return (x0+3)**4 + (x1-1)**2 + (x2-2)**2 + (x3+x0)**2
def grad_fun4(x):
    x0, x1, x2, x3 = x
    return [4*(x0+3)**3 + 2*(x3+x0),
            2*(x1-1), 2*(x2-2), 2*(x3+x0)]
def hessian_fun4(x):
    x0, x1, x2, x3 = x
    return [[12*(x0+3)**2 + 2,0,0,2],
            [0,2,0,0], [0,0,2,0], [2,0,0,2]]
\end{lstlisting}

\subsubsection{Проверка выпуклости}
\begin{lstlisting}[language=Python]
fun4_x_range = np.array(np.meshgrid(x, x, x, x)).T.reshape(-1, 4)
is_convex = check_convexity(fun4, grad_fun4, hessian_fun4, fun4_x_range)
print(f"fun4(x, y, z, k) is convex: {is_convex}")
\end{lstlisting}

Вывод: fun4(x, y, z, k) выпуклая: True

Начальную точку возьмем (-1,0.5,0.5,0.5)

\begin{lstlisting}[language=Python]
fun4_x0 = [-1,0.5,0.5,0.5]
\end{lstlisting}

\subsubsection{Применение МСГ}

\begin{lstlisting}[language=Python]
res4 = conjugate_gradient5(fun4, grad_fun4, fun4_x0)
\end{lstlisting}

\newpage


\subsection{Пример 5}

\subsubsection{Реализация}
\begin{lstlisting}[language=Python]
def fun5(x):
    x0,x1,x2,x3=x
    return
     -2*x0**3+4*x0**2+x1**4-3*x1**3-4*x2**3+2*x2**4-5*x3+x3**2+10+x0+x1+x2+x3
def grad_fun5(x):
    x0,x1,x2,x3=x
    return [-6*x0**2+8*x0+1,  4*x1**3-9*x1**2+1,
            -12*x2**2+8*x2**3+1,    -5+2*x3+1]
def hessian_fun5(x):
    x0,x1,x2,x3=x
    return [[-12*x0+8,0,0,0],   [0,12*x1**2-18*x1,0,0],
            [0,0,-24*x2+24*x2**2,0],  [0,0,0,2]]
\end{lstlisting}

\subsubsection{Проверка выпуклости}
\begin{lstlisting}[language=Python]
fun5_x_range = np.array(np.meshgrid(x, x, x, x)).T.reshape(-1, 4)
is_convex = check_convexity(fun5, grad_fun5, hessian_fun5, fun5_x_range)
print(f"fun5(x, y, z, k) is convex: {is_convex}")
\end{lstlisting}

Вывод: fun5(x, y, z, k) is convex: False

Начальную точку возьмем (-0.5,2.5,1.5,2.0)

\begin{lstlisting}[language=Python]
fun5_x0=[-0.5,2.5,1.5,2.0]
\end{lstlisting}

\subsubsection{Применение МСГ}

\begin{lstlisting}[language=Python]
res5 = conjugate_gradient5(fun5, grad_fun5, fun5_x0)
\end{lstlisting}

\newpage
\section{Результаты работы программы на примерах}
\subsection{Результаты}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{results.png}
    \caption{Процесс оптимизации для примеров}
\end{figure}
\newpage
\subsection{Графики}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.3\textwidth]{graph.png}
    \caption{Процесс оптимизации для примеров}
\end{figure}

\newpage
\section{Выводы}
Пример 1: выпуклая функция, минимум найден успешно

Пример 2,3: минимум не был найден - решение ушло на - бесконечность

Пример 4: минимум найден успешно

Пример 5: если взять точку, далекую от решения, то решение не будет найдено - функция завершится из-за не найденного alpha. Если подобрать точку, близкую к минимуму, то программа успешно находит минимум, как показано на картинках.


В данном отчете была продемонстрирована реализация метода сопряженных градиентов для оптимизации различных функций. Полученное решение подтверждает эффективность метода только для выпуклых функций. Визуализация показывает процесс сходимости к оптимальному решению, можем сделать вывод, что сходимость сверхлинейная.
\end{document}
