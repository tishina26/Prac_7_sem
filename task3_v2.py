# -*- coding: utf-8 -*-
"""Yet another copy of Задание 1 прак 7 семестр

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zOEYB_2ODtclWVUUlgt03Wurckh2sUa7

Импорт библиотек
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

"""#Задание функций, градиентов и гессианов

fun = $-2x_0^3+4x_0^2+x_1^4-3x_1^3-4x_2^3+2x_2^4-5x_3+x_3^2+10+x_0+x_1+x_2+x_3$
"""

def fun(x):
    # print(x)
    x0,x1,x2,x3=x
    return -2*x0**3+4*x0**2+x1**4-3*x1**3-4*x2**3+2*x2**4-5*x3+x3**2+10+x0+x1+x2+x3

"""# Реализация действий с матрицами"""

# перемножение числа и вектора
def mult(a, arr):
    return np.array([i*a for i in arr])

# сложение двух векторов
def add(arr1, arr2):
    try:
        return np.array([arr1[i]+arr2[i] for i in range(len(arr1))])
    except TypeError:
        arr1+arr2[0]

# Скалярное произведение
def mydot(arr1, arr2):
    try:
        ans=0
        for i in range(len(arr1)):
            ans+=arr1[i]*arr2[i]
        return ans
    except:
        return arr1*arr2[0]
# взятие нормы вектора
def norm(arr):
    return sum(i**2 for i in arr)**0.5

# реализация численного приближения градиента
def my_grad(f, x, h=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        xh = x.copy()
        x_h = x.copy()
        xh[i] += h
        x_h[i] -= h
        grad[i] = (f(xh) - f(x_h))/h/2
    return grad

"""# Реализация метода сопряженных градиентов"""

def find_alpha(f, a,b, tol=1e-6, max_iter = 100):
    phi = (1 + 5**0.5) / 2
    resphi = 2 - phi

    x1 = a + resphi * (b - a)        # начальные точки
    x2 = b - resphi * (b - a)
    # print(x1,x2)
    f1 = f(x1)
    f2 = f(x2)
    # print(f1,f2)

    for _ in range(max_iter):
        # print(x1,x2)
        if abs(b - a) < tol:
            break
        # print("f1f2",f1,f2)
        if f1 < f2:
            b, x2, f2 = x2, x1, f1
            x1 = a + resphi * (b - a)
            f1 = f(x1)
        else:
            a, x1, f1 = x1, x2, f2
            x2 = b - resphi * (b - a)
            f2 = f(x2)


    return (a + b) / 2

def conjugate_gradient5(f, x0, tol=1e-5, max_iter=1000, alpha_max=1.0, c=0.5, rho=0.2, alpha_min=1e-10, max_f = 1e15, end_fun="norm_grad"):
    """
    Метод сопряженных градиентов с backtracking line search.

    Args:
    f: Функция, для которой нужно найти минимум.
    grad_f: Градиент функции.
    x0: Начальная точка поиска.
    tol: Допустимая погрешность.
    max_iter: Максимальное количество итераций.
    alpha_max: Максимальная длина шага.
    c: Параметр для backtracking line search.
    rho: Параметр для backtracking line search.
    alpha_min: Минимальная длина шага.
    max_f: Максимально допустимое значение f, иначе считаем = null.

    Returns:
    ans = {"message", "status", "nit", "x", "f_min": arr_f[-1], "arr_f": arr_f}
    """
    arr_f = [f(x0)]
    x = x0
    r = mult(-1, my_grad(f, x))
    d = r

    ans = {"message": "not enough iterations",
           "status": 3,
           "nit": 0,
           "x": x,
           "f_min": arr_f[-1],
           "arr_f": arr_f}

    for i in range(max_iter):
        ans["nit"]+=1
        alpha = alpha_max
        # Продвинутый поиск по линии с обратным шагом
        while f(add(x, mult(alpha, d))) > f(x) + c * alpha * mydot(my_grad(f, x), d) and alpha > alpha_min:
            alpha *= rho
        # def f_new(alpha): return f(add(x, mult(alpha, d)))
        # alpha = find_alpha(f_new, -c,c,tol)

        if abs(f(x))>=max_f:
            ans["message"] = 'too big f or too small'
            ans["x"], ans["f_min"], ans["arr_f"], ans["status"] = x, arr_f[-1], arr_f, 1
            return ans
        if alpha < alpha_min:  # Если шаг слишком мал, прерываем
            ans["message"] = 'too small alpha'
            ans["x"], ans["f_min"], ans["arr_f"], ans["status"] = x, arr_f[-1], arr_f, 2
            return ans

        x = add(x, mult(alpha, d))
        arr_f.append(f(x))
        r_new = mult(-1, my_grad(f, x))
        beta = mydot(r_new, r_new) / mydot(r, r)
        # beta = norm(r_new)**2 / norm(r)**2
        d = add(r_new, mult(beta, d))
        r = r_new

        # Проверка условия сходимости
        if end_fun=="norm_grad":
            if norm(r) < tol:
                ans["message"] = 'ok'
                ans["status"] = 0
                break
        elif end_fun=="dif_f":
            if len(arr_f)>2:
                if abs(arr_f[-1]-arr_f[-2])<tol:
                    ans["message"] = 'ok'
                    ans["status"] = 0
                    break
        elif end_fun=="norm_f":
            if len(arr_f)>2:
                if (arr_f[-1]-arr_f[-2])**2<tol:
                    ans["message"] = 'ok'
                    ans["status"] = 0
                    break
    print(end_fun)
    ans["x"], ans["f_min"], ans["arr_f"] = x, arr_f[-1], arr_f
    return ans

"""#Применение метода сопряженных градиентов

## Зависимость решения от начальной точки
"""

res_arr = []
res_name = []

for i in range(-500,500, 2):
    j=i/10
    fun_x0 = [j,j,j,j]
    res_arr.append(conjugate_gradient5(fun,fun_x0))
    res_name.append(str(i))
    # print(res_arr[-1])
    res_arr[-1]["right_anser"]=minimize(fun, x0=np.array(fun_x0), method='CG')
    res_arr[-1]["x0"] = fun_x0

print("|--------x0--------|-----------------------x_min-----------------------|--------f_min-------|--nit--|-status-|--message--|----righ_answer_f|--rf_s--|")
for i in range(len(res_arr)):
    n = res_name[i]
    r = res_arr[i]
    x0, x_min, f, arr_f, s, nit, m,rr = r["x0"], r["x"], r["f_min"], r["arr_f"], r["status"],r["nit"],r["message"],r["right_anser"]
    rf, rs = rr["fun"], rr["status"]

    if s==0:    print(f"| {x0} | {x_min} | {f} | {nit} | {s} | {m} | {rf} | {rs} |")

"""везде, где мой метод завершился усешно, встроенный метод тоже завершился успешно (крме первой строки), значение минимума совпадает с моей функцией

## Зависимость решения от длины шага при точности = 0,0001
"""

res_arr = []
res_name = []
fun_x0 = [0,0,0,0]
# fun_x0 = [3,3,3,3]
for i in range(1,100, 1):
    rho = i/100
    res_arr.append(conjugate_gradient5(fun,fun_x0, rho=rho,tol=0.0001))
    res_name.append(str(i))
    # print(res_arr[-1])
    res_arr[-1]["right_anser"]=minimize(fun, x0=np.array(fun_x0), method='CG')
    res_arr[-1]["rho"] = rho

print("|--rho--|-----------------------x_min-----------------------|--------f_min-------|--nit--|-status-|--message--|")
rho_res=[]
nit_res=[]
for i in range(len(res_arr)):
    n = res_name[i]
    r = res_arr[i]
    rho, x_min, f, arr_f, s, nit, m,rr = r["rho"], r["x"], r["f_min"], r["arr_f"], r["status"],r["nit"],r["message"],r["right_anser"]

    if s==0:
        print(f"| {rho} | {x_min} | {f} | {nit} | {s} | {m} |")
        rho_res.append(rho)
        nit_res.append(nit)

plt.plot(rho_res, nit_res, marker='o', linestyle='-')
plt.xlabel("rho")
plt.ylabel("nit")
plt.title("nit vs. rho")
plt.grid(True)
plt.show()

"""## Зависимость точности от выбранного метода остановки"""

res_arr3 = []
res_name3 = []

stop_fun= ["norm_grad","norm_f","dif_f"]
fun_x0=[0.0,0.0,0.0,0.0]
for i in stop_fun:
    res_arr3.append(conjugate_gradient5(fun,fun_x0,tol=1e-7,rho=0.4,end_fun=i))
    res_name3.append(str(i))
    # print(res_arr3[-1])
    res_arr3[-1]["x0"] = fun_x0
print(minimize(fun, x0=np.array(fun_x0), method='CG'))
    #res_arr3[-1]["x0"] = fun_x0

ii=0
for i in res_arr3:
    print("_"*30)
    print(res_name3[ii])
    ii+=1
    print("_"*30)
    for j in i:
        print(j, i[j])